{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preprocessing import remove_punct_num\n",
    "from preprocessing import remove_stopwords\n",
    "from preprocessing import stemmer2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(*args):\n",
    "    X_train, X_test = args[2][args[0]], args[2][args[1]]\n",
    "    y_train, y_test = args[3][args[0]], args[3][args[1]]\n",
    "    \n",
    "    # count\n",
    "    training = [0,0,0,0,0,0,0,0]\n",
    "    testing = [0,0,0,0,0,0,0,0]\n",
    "\n",
    "    for y in y_train:\n",
    "        training[y] += 1\n",
    "\n",
    "    for y in y_test:\n",
    "        testing[y] += 1\n",
    "\n",
    "    print \"Training Set:\" , training\n",
    "    print \"Testing Set:\" , testing\n",
    "    #     \n",
    "    \n",
    "     # tf-idf vectorizer \n",
    "    if (int(args[4]) == 1):\n",
    "        vec =  binary(X_train)\n",
    "        vectorizer = vec[0]\n",
    "        train_data_features = vec[1]\n",
    "    elif (int(args[4]) == 2):\n",
    "        vec = word_count(X_train)\n",
    "        vectorizer = vec[0]\n",
    "        train_data_features = vec[1]\n",
    "    else:\n",
    "        vec = tf_idf(X_train)\n",
    "        vectorizer = vec[0]\n",
    "        train_data_features = vec[1]\n",
    "    \n",
    "    # svm\n",
    "    if int(args[5]) == 1:\n",
    "        clf = run_svm(train_data_features,y_train)\n",
    "    else:\n",
    "        clf = run_mnb(train_data_features,y_train_)\n",
    "\n",
    "    #\n",
    "    X_testArray = X_test.values\n",
    "    y_testArray = y_test.values\n",
    "    \n",
    "    #accuracy\n",
    "    test_data_features = vectorizer.transform(X_test)\n",
    "    pred = clf.predict(test_data_features)\n",
    "    accuracy = clf.score(test_data_features,y_test)\n",
    "    print \"accuracy:\", accuracy\n",
    "    \n",
    "    f1 = f1_score(y_testArray,pred, average=None)\n",
    "    avg = np.sum(f1)/8\n",
    "    print \"f-score: \", f1\n",
    "    print \"avg:\", avg\n",
    "    \n",
    "    print \"\\n\\n\"\n",
    "    return (clf, vectorizer, f1, avg, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction\n",
      "1.Binary\n",
      "2.Word Count\n",
      "3.TF-IDF\n",
      "Choose: 3\n",
      "Classifier\n",
      "1.SVM\n",
      "2.Multinomial Naive Bayes\n",
      "Choose: 1\n",
      "\n",
      "\n",
      "Iteration:  0\n",
      "170     5.0\n",
      "171     7.0\n",
      "172     6.0\n",
      "173     NaN\n",
      "174     5.0\n",
      "175     3.0\n",
      "176     4.0\n",
      "177     3.0\n",
      "178     7.0\n",
      "179     5.0\n",
      "180     6.0\n",
      "181     5.0\n",
      "182     5.0\n",
      "183     1.0\n",
      "184     NaN\n",
      "185     3.0\n",
      "186     5.0\n",
      "187     5.0\n",
      "188     NaN\n",
      "189     1.0\n",
      "190     2.0\n",
      "191     6.0\n",
      "192     NaN\n",
      "193     0.0\n",
      "194     1.0\n",
      "195     5.0\n",
      "196     NaN\n",
      "197     4.0\n",
      "198     NaN\n",
      "199     1.0\n",
      "       ... \n",
      "1666    2.0\n",
      "1667    2.0\n",
      "1668    0.0\n",
      "1669    NaN\n",
      "1670    1.0\n",
      "1671    NaN\n",
      "1672    7.0\n",
      "1673    0.0\n",
      "1674    7.0\n",
      "1675    0.0\n",
      "1676    NaN\n",
      "1677    1.0\n",
      "1678    2.0\n",
      "1679    4.0\n",
      "1680    NaN\n",
      "1681    5.0\n",
      "1682    0.0\n",
      "1683    1.0\n",
      "1684    6.0\n",
      "1685    6.0\n",
      "1686    1.0\n",
      "1687    5.0\n",
      "1688    6.0\n",
      "1689    5.0\n",
      "1690    NaN\n",
      "1691    1.0\n",
      "1692    1.0\n",
      "1693    7.0\n",
      "1694    NaN\n",
      "1695    2.0\n",
      "Name: multiclass, dtype: float64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not numpy.float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-a98073e05d6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Iteration: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mclf_est\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-da58f6bbc238>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not numpy.float64"
     ]
    }
   ],
   "source": [
    "\n",
    "#df = pd.read_csv('../data/experiment1/shuffled_updated.csv')\n",
    "#df = pd.read_csv('../data/no_stemming.csv')\n",
    "df = pd.read_csv('../data/experiment1/shuffled1.csv')\n",
    "\n",
    "X, y = df['content'], df['multiclass']\n",
    "\n",
    "X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.20, random_state = 42)\n",
    "clf_est = []\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "feat = raw_input(\"Feature Extraction\\n1.Binary\\n2.Word Count\\n3.TF-IDF\\nChoose: \")\n",
    "c = raw_input(\"Classifier\\n1.SVM\\n2.Multinomial Naive Bayes\\nChoose: \")\n",
    "print \"\\n\"\n",
    "\n",
    "i = 0\n",
    "for train_index, test_index in kf.split(X_train_):\n",
    "    print \"Iteration: \", i\n",
    "    clf_est.append(evaluate(train_index,test_index, X_train_, y_train_, feat, c))\n",
    "    i += 1\n",
    "    \n",
    "best_estimator = max(clf_est,key=itemgetter(3))\n",
    "print best_estimator[0], best_estimator[2], best_estimator[3]\n",
    "\n",
    "clf = best_estimator[0]\n",
    "vectorizer = best_estimator[1]\n",
    "X_train = best_estimator[4]\n",
    "y_train = best_estimator[5]\n",
    "X_test = best_estimator[6]\n",
    "y_test = best_estimator[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(*args):\n",
    "    vectorizer = TfidfVectorizer(min_df=1) # max_features=10000, binary=True, norm=None, use_idf=False\n",
    "    X= vectorizer.fit_transform(args[0]) \n",
    "    train_data_features = X.toarray()\n",
    "\n",
    "    print train_data_features.shape\n",
    "    return (vectorizer,train_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def binary(*args):\n",
    "    vectorizer = TfidfVectorizer(min_df=1, binary=True, norm=None, use_idf=False) #max_features=30000\n",
    "    X= vectorizer.fit_transform(args[0]) \n",
    "    train_data_features = X.toarray()\n",
    "\n",
    "    print train_data_features.shape\n",
    "    return (vectorizer,train_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_count(*args):\n",
    "    vectorizer = CountVectorizer(min_df=1, max_features=30000) #, binary=True, norm=None, use_idf=False\n",
    "    X= vectorizer.fit_transform(args[0]) \n",
    "    train_data_features = X.toarray()\n",
    "\n",
    "    print train_data_features.shape\n",
    "    return (vectorizer,train_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_svm(*args):\n",
    "    clf = OneVsRestClassifier(LinearSVC(C=1.0,random_state=42)) # C=1.0, max_iter=1000\n",
    "    clf.fit(args[0],args[1])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_mnb(*args):    \n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(args[0],args[1])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.929245283019\n"
     ]
    }
   ],
   "source": [
    "test_data_features = vectorizer.transform(X_test)\n",
    "pred = clf.predict(test_data_features)\n",
    "accuracy = clf.score(test_data_features,y_test)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0=crime 1=disaster 2=entertainment 3=economic 4=health 5=political 6=sports 7=terrorism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print pred\n",
    "# print pred.shape\n",
    "# print y_test.values\n",
    "\n",
    "X_testArray = X_test.values\n",
    "y_testArray = y_test.values\n",
    "\n",
    "# for i in range(len(X_test)):\n",
    "#     print X_testArray[i] \n",
    "#     print \"label: \", y_testArray[i] \n",
    "#     print \"predicted: \", pred[i]\n",
    "#     print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30  0  0  0  0  0  0  0]\n",
      " [ 1 31  0  0  0  0  0  1]\n",
      " [ 0  0 36  0  0  0  0  0]\n",
      " [ 0  0  0 16  0  1  0  0]\n",
      " [ 0  1  0  1  8  0  0  0]\n",
      " [ 2  0  0  0  0 38  0  1]\n",
      " [ 0  0  0  0  0  1 26  0]\n",
      " [ 3  0  0  0  0  3  0 12]]\n"
     ]
    }
   ],
   "source": [
    "# row : actual :: column : predicted \n",
    "cf = confusion_matrix(y_testArray,pred)\n",
    "print cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime\n",
      "30 0 6 176\n",
      "Precision:  0.833333333333\n",
      "Recall:  1.0\n",
      " \n",
      "Disaster\n",
      "31 2 1 178\n",
      "Precision:  0.96875\n",
      "Recall:  0.939393939394\n",
      " \n",
      "Entertainment\n",
      "36 0 0 176\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      " \n",
      "Economic\n",
      "16 1 1 194\n",
      "Precision:  0.941176470588\n",
      "Recall:  0.941176470588\n",
      " \n",
      "Health\n",
      "8 2 0 202\n",
      "Precision:  1.0\n",
      "Recall:  0.8\n",
      " \n",
      "Political\n",
      "38 3 5 166\n",
      "Precision:  0.883720930233\n",
      "Recall:  0.926829268293\n",
      " \n",
      "Sports\n",
      "26 1 0 185\n",
      "Precision:  1.0\n",
      "Recall:  0.962962962963\n",
      " \n",
      "Terrorism\n",
      "12 6 2 192\n",
      "Precision:  0.857142857143\n",
      "Recall:  0.666666666667\n",
      " \n",
      "pre:  0.935515448912\n",
      "rec: 0.904628663488\n",
      "hey  197 15 15 1469 0.982311320755\n"
     ]
    }
   ],
   "source": [
    "# 8 total classes\n",
    "from __future__ import division\n",
    "\n",
    "total = 8\n",
    "classes = {0:'Crime', 1:'Disaster', 2:'Entertainment', 3:'Economic', 4:'Health', 5:'Political',\n",
    "          6:'Sports', 7:'Terrorism'}\n",
    "category = 0\n",
    "\n",
    "TrueP = []\n",
    "FalseN = []\n",
    "FalseP = []\n",
    "TrueN = []\n",
    "\n",
    "pre = 0\n",
    "rec = 0\n",
    "\n",
    "for category in range(total):\n",
    "    TP = FN = FP = TN = 0 \n",
    "    for i in range(total):\n",
    "        for j in range(total):\n",
    "            if i==category and j==category:\n",
    "                TP += cf[i][j]\n",
    "            elif i==category and j!=category:\n",
    "                FN += cf[i][j]\n",
    "            elif i!=category and j==category:\n",
    "                FP += cf[i][j]\n",
    "            else:\n",
    "                TN += cf[i][j]\n",
    "    print classes.get(category)\n",
    "    print TP, FN, FP, TN\n",
    "    print \"Precision: \" , TP/(TP+FP)\n",
    "    print \"Recall: \" , TP/(TP+FN)\n",
    "    print \" \"\n",
    "    TrueP.append(TP)\n",
    "    FalseN.append(FN)\n",
    "    FalseP.append(FP)\n",
    "    TrueN.append(TN)\n",
    "    pre += (TP/(TP+FP))\n",
    "    rec += TP/(TP+FN)\n",
    "\n",
    "print \"pre: \", pre/8\n",
    "print \"rec:\", rec/8\n",
    "    \n",
    "# Average confusion matrix for all classes\n",
    "TP = np.sum(TrueP)\n",
    "FN = np.sum(FalseN)\n",
    "FP = np.sum(FalseP)\n",
    "TN = np.sum(TrueN)  \n",
    "\n",
    "print  \"hey \",  TP, FN, FP, TN, ((TP+TN)/(TP+TN+FP+FN))\n",
    "\n",
    "# PPV = TP / (TP + FP) \n",
    "# TPR = TP / (TP + FN)\n",
    "# Fscore = 2 * ((PPV * TPR)/(PPV + TPR))\n",
    "\n",
    "# print \" \"\n",
    "# print \"Precision: \" , PPV\n",
    "# print \"Recall: \" , TPR\n",
    "# print \"F-score: \" , Fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.90909091  0.95384615  1.          0.94117647  0.88888889  0.9047619\n",
      "  0.98113208  0.75      ]\n",
      "\n",
      "\n",
      "0.916112050331\n"
     ]
    }
   ],
   "source": [
    "# calculate f-score for each category\n",
    "f1 = f1_score(y_testArray,pred, average=None)\n",
    "\n",
    "print f1\n",
    "print \"\\n\"\n",
    "print np.sum(f1)/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score\n",
      "-----\n",
      "Weighted:  0.927945015903\n",
      "Micro:  0.929245283019\n",
      "Macro:  0.916112050331\n"
     ]
    }
   ],
   "source": [
    "# calculate f-score for all \n",
    "print \"F-score\\n-----\"\n",
    "print \"Weighted: \", f1_score(y_testArray,pred, average='weighted')\n",
    "print \"Micro: \", f1_score(y_testArray,pred, average='micro')\n",
    "print \"Macro: \", f1_score(y_testArray,pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Crime       0.83      1.00      0.91        30\n",
      "     Disaster       0.97      0.94      0.95        33\n",
      "Entertainment       1.00      1.00      1.00        36\n",
      "     Economic       0.94      0.94      0.94        17\n",
      "       Health       1.00      0.80      0.89        10\n",
      "    Political       0.88      0.93      0.90        41\n",
      "       Sports       1.00      0.96      0.98        27\n",
      "    Terrorism       0.86      0.67      0.75        18\n",
      "\n",
      "  avg / total       0.93      0.93      0.93       212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Crime', 'Disaster', 'Entertainment', 'Economic','Health','Political','Sports','Terrorism']\n",
    "print(classification_report(y_testArray, pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save to pickle\n",
    "import pickle\n",
    "\n",
    "with open('categorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "    \n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REALLY NEW DATA OMGGGGG!!\n",
    "#UAAP: Ateneo pilay kontra FEU By Elech Dawa March 9 na news ni siya\n",
    "arr = \"target ateneo first round sweep harap far eastern university ngayon araw uaap season mens football tournament moro lorenzo field sikap blue eag­les  dagit sweep wala league-leading goalscorer jarvey gayoso nabigyan si sopho­more striker gayoso five goals season dalawa yellow cards suspendido laro kontra tamaraws kaldag una laro ust nu salo fourth place points kritikal laro dalawa habol sa final four bayani gayoso rookies sam lim enzo ceniza may maximum points ateneo lima puntos una segundong up feu tigatlong goals isasalpak ngayong season lim at ceniza nais tamaraws balik tikas depensa laban blue eagles\"\n",
    "testing = vectorizer.transform([arr])\n",
    "clf.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter:Suicide bombing sa Cebu kahapon\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = raw_input(\"Enter:\")\n",
    "a = remove_punct_num.removePunctuationAndNumbers(a)\n",
    "a = remove_stopwords.remove_stopwords(a)\n",
    "a = stemmer2.stem(a)\n",
    "testing = vectorizer.transform([a])\n",
    "clf.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
